{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aprendizagem não supervisionada.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kur1sutaru/balaio-de-scripts/blob/add-license-1/Aprendizagem_n%C3%A3o_supervisionada.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBVRISpApoLL"
      },
      "source": [
        "# Aprendizagem não supervisionada"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plt8liR5poLP"
      },
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5A7A2dIpoLT"
      },
      "source": [
        "# Introdução\n",
        "Nesse notebook, analisaremos inicialmente alguns dados do **Viroma**, que consiste em um teste metagenômico para identificação de vírus de RNA em amostras clínicas implementado no Hospital Albert Einstein. Após o processamento da amostra no wet lab e obtenção das sequências por NGS, os dados passam por diversas etapas de controle de qualidade e análise, gerando com resultado final um plot semelhante a este:\n",
        "\n",
        "![](https://www.researchgate.net/publication/340000406/figure/fig1/AS:870415593377825@1584534657597/Krona-plot-representing-an-overview-of-all-viral-sequences-identified-by-Illumina.ppm)\n",
        "\n",
        "Na versão original, esse é um gráfico interativo, permitindo navegar entre os diferentes táxons e visualizar o número de sequências obtidos em cada um. A partir desse resultado, os especialistas do laboratório observam as espécies mais prevalentes e definem o diagnóstico.\n",
        "\n",
        "Para maiores informações sobre o escopo desse teste, visite [este post do blog do Varstation](https://varstation.com/pt/blog/artigos/viroma-a-medicina-de-precisao-que-chegou-de-vez-no-diagnostico-e-tratamento-de-doencas-infecciosas-virais/).\n",
        "\n",
        "# Importação dos dados\n",
        "\n",
        "Os dados devidamente anonimizados de diversas amostras de Viroma se encontram em um arquivo CSV. Nesse arquivo, cada coluna a partir da terceira representa a abundância relativa de um gênero encontrado nas amostras. Vamos agora importar esse arquivo e fazer algumas análises exploratórias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_FdZQ-GpoLT"
      },
      "source": [
        "!wget https://dados-anonimizados-viroma.s3-sa-east-1.amazonaws.com/dados_viroma_anonimizados.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFOyvg-HpoLT"
      },
      "source": [
        "df = pd.read_csv(\"dados_viroma_anonimizados.csv\", index_col=\"ID\")\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8IPAQ53poLU"
      },
      "source": [
        "Podemos inspecionar quais são os gêneros mais abundantes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbRiHduZpoLU"
      },
      "source": [
        "df.mean().sort_values(ascending=False).head(10).plot(kind=\"barh\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rad11xH4poLV"
      },
      "source": [
        "df.median().sort_values(ascending=False).head(10).plot(kind=\"barh\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dYe74WEEpoLW"
      },
      "source": [
        "Temos diversos tipos de amostra no dataset, com predominância de plasma, swab e líquor. Vamos colocar as amostras dos demais materiais na categoria genérica \"Outros\", para reduzir o número de categorias nessa variável:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoUsbThapoLW"
      },
      "source": [
        "df[\"Material\"].value_counts().plot(kind=\"barh\")\n",
        "plt.title(\"Tipos de amostra por material\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZEIeh3KpoLW"
      },
      "source": [
        "df[\"Material\"].mask(~df[\"Material\"].isin([\"Plasma\", \"Liquor\", \"Swab\"]), \"Outros\", inplace=True)\n",
        "df[\"Material\"].value_counts().plot(kind=\"barh\")\n",
        "plt.title(\"Tipos de amostra por material\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAOV2X8gpoLX"
      },
      "source": [
        "Temos também muitas amostras nas quais nenhum patógeno foi identificado. Vamos verificar quantos são esses resultados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ceBh-fTpoLX"
      },
      "source": [
        "print(\"Resultados negativos:\\t{0:.1%}\".format((df[\"Resultado\"]==\"Negativo\").mean()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-8K_pfypoLX"
      },
      "source": [
        "Alguns gêneros desse dataset são da microbiota natural, enquanto outros são patógenos. Os perfis desses dois grupos tendem a ser bem diferentes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXJXtFecpoLX"
      },
      "source": [
        "\"\"\"plt.hist(df[\"Sphingomonas\"][df[\"Material\"]==\"Plasma\"], bins=30)\n",
        "plt.title(\"Distribuição da abundância relativa de Sphingomonas\\n(bactéria da microbiota normal)\")\n",
        "plt.xlabel(\"Abundância\")\n",
        "plt.ylabel(\"Frequência\");\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xy3ufiImpoLY"
      },
      "source": [
        "plt.hist(df[\"Orthohepadnavirus\"][df[\"Material\"]==\"Plasma\"], bins=30)\n",
        "plt.title(\"Distribuição da abundância relativa de Orthohepadnavirus\\n(vírus da hepatite B)\")\n",
        "plt.xlabel(\"Abundância\")\n",
        "plt.ylabel(\"Frequência\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aN2kr2VppoLY"
      },
      "source": [
        "# Redução de dimensionalidade com PCA\n",
        "O método mais clássico de redução de dimensionalidade é o **PCA (Principal Components Analysis)**. Nessa técnica, o objetivo é projetar o dataset em um espaço de dimensão pequena, mas de maneira que seja possível reconstruir o dataset original da maneira mais precisa possível. A figura a seguir, extraída [deste link](https://fspanero.wordpress.com/2009/12/30/analise-de-componente-principais-pca/), ilustra o princípio dessa técnica:\n",
        "\n",
        "![](https://fspanero.files.wordpress.com/2009/12/pca.jpg)\n",
        "\n",
        "Ao tomar um número muito pequeno de componentes principais, podemos perder características importantes do dataset. Por outro lado, escolher um número muito grande de componentes é contraproducente, já que vai contra o propósito da redução de dimensionalidade. Uma forma de evitar essa escolha  arbitrária do número de componentes é definir uma proporção de variância explicada, de modo que o próprio algoritmo escolha o número de componentes que seja necessário para capturar as principais características do dataset.\n",
        "\n",
        "Vamos por isso em prática com a biblioteca **Scikit-Learn**, que contém implementações de diversos algoritmos de ML. Podemos especificar que queremos um número de dimensões suficiente para capturar 95% da variância do dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz9iql1qpoLY"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(0.95)\n",
        "princ_comps = pca.fit_transform(np.log10(df.iloc[:,2:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56S3PF1PpoLZ"
      },
      "source": [
        "Podemos fazer um gráfico para ver a proporção da variância que é alocada em cada componente:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GquFfd_7poLZ"
      },
      "source": [
        "plt.plot(1+np.arange(pca.n_components_), np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.plot(1+np.arange(pca.n_components_), pca.explained_variance_ratio_)\n",
        "plt.xlim(1, pca.n_components_)\n",
        "plt.ylim(0,1)\n",
        "plt.xlabel(\"Número de componentes\")\n",
        "plt.ylabel(\"Fração de variância explicada\")\n",
        "plt.grid(True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjh_5hUnpoLa"
      },
      "source": [
        "*Obs.: em geral, é importante que o dataset passe por ume etapa de scaling, onde cada feature é normalizada para que tenham a mesma variabilidade (em geral medida por desvio padrão ou pela diferença entre valor máximo e mínimo). Nesse dataset, essa normalização já foi feita antes por outro método.*\n",
        "\n",
        "Podemos ver graficamente as duas primeiras componentes principais. Note que PC2 separa razoavelmente as amostras de plasma e de swab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTgeX9tApoLa"
      },
      "source": [
        "plt.figure(figsize=(10,7))\n",
        "sns.scatterplot(x=princ_comps[:,0], y=princ_comps[:,1], hue=df[\"Material\"])\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.title(\"Redução de dimensionalidade com PCA\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOXqeiM_poLa"
      },
      "source": [
        "Sabemos que as primeiras componentes concentram a maior parte da variabilidade dos dados. Dessa forma, podemos remover a variação populacional normal subtraindo essas componentes, configurando o chamado **PCA denoising**. O que sobra dos dados então é aquilo que é mais incomum nas amostras, e possivelmente de maior interesse clínico. Vamos chamar essa diferença entre um datapoint e sua projeção no hiperplano das primeiras PCs de erro de reconstrução."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj9ks2xSpoLa"
      },
      "source": [
        "proj = pca.inverse_transform(princ_comps)\n",
        "err = np.log10(df.iloc[:,2:]) - proj\n",
        "err"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8P22xMypoLa"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.hist(err.values.flatten(), bins=50)\n",
        "plt.axvline(err.max().max(), ls=\"--\", c=\"k\", label=\"Mínimo e máximo\")\n",
        "plt.axvline(err.min().min(), ls=\"--\", c=\"k\")\n",
        "plt.axvline(np.percentile(err.values.flatten(), 1), ls=\"--\", c=\"gray\", label=\"1º e 99º percentis\")\n",
        "plt.axvline(np.percentile(err.values.flatten(), 99), ls=\"--\", c=\"gray\")\n",
        "plt.xlabel(\"Erro de reconstrução\")\n",
        "plt.ylabel(\"Frequência\")\n",
        "plt.title(\"Distribuição do erro de reconstrução\")\n",
        "plt.legend()\n",
        "plt.show();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZdXEbWZpoLa"
      },
      "source": [
        "pd.DataFrame({\n",
        "    \"Gênero mais alterado\":err.T.apply(lambda v: v.index[np.argmax(v)]),\n",
        "    \"Erro de reconstrução\":err.T.apply(max),\n",
        "    \"Resultado\":df[\"Resultado\"]\n",
        "}).sort_values(by=\"Erro de reconstrução\", ascending=False).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AZG50sFpoLa"
      },
      "source": [
        "sns.violinplot(\n",
        "    y=err.max(axis=1).rename(\"Máximo erro de reconstrução\"),\n",
        "    x=(df[\"Resultado\"]!=\"Negativo\").map({True:\"Positivo\", False:\"Negativo\"}));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MnBBn8nwpoLa"
      },
      "source": [
        "## Redução de dimensionalidade com outros métodos: Manifold Learning\n",
        "\n",
        "Implicitamente, o PCA assume que os dados se concentram em torno de um hiperplano, o que torna as projeções lineares que esse método faz muito eficientes. Porém, muitas vezes os dados se aproximam de uma superfície não-linear, que se enquadra no conceito matemático de **manifold** (ou *variedade*, em português). Entre as técnicas de Manifold Learning, estão Isomap e T-SNE.\n",
        "\n",
        "O Isomap é muito parecido com o PCoA/MDS: é um método que tenta preservar as distâncias entre os pontos do dataset. A diferença (que torna o Isomap geralmente melhor que o PCoA) é que nesse algoritmo as distâncias são calculadas ao longo da manifold, utilizando o grafo de $k$ vizinhos mais próximos. Veja a figura a seguir que ilustra esse processo:\n",
        "\n",
        "- **A.** Dados no espaço original\n",
        "- **B.** Rede de vizinhos mais próximos e cálculo de distâncias\n",
        "- **C.** Espaço de dimensão reduzida, no qual as distâncias euclidianas refletem as distâncias geodésicas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bj8uBuUspoLa"
      },
      "source": [
        "![](https://benalexkeen.com/wp-content/uploads/2017/05/isomap.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8kqwYm5poLb"
      },
      "source": [
        "Já o T-SNE é um método totalmente desenhado para aumentar a separação entre os clusters em um dataset. Esse algoritmo aplica diversas heurísticas baseadas em entidades como a distribuição T-Student e divergência KL. Vale destacar algumas características desse método:\n",
        "- É um método **estocástico** e propenso a encontrar **mínimos locais**, então executar o algoritmo duas vezes nem sempre dá resultados reprodutíveis.\n",
        "- Esse método garante que pontos próximos continuem próximos, mas não garante que pontos distantes continuem distantes (ou seja, pode ocorrer a **fusão de clusters**, mas raramente a separação de um cluster).\n",
        "- Na Bioinformática, esse método foi abraçado pela comunidade de **scRNAseq**, funcionando muito bem para separar células por padrões de expressão gênica. Veja a seguir um exemplo de T-SNE plot nesse tipo de dado:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m377vz3poLb"
      },
      "source": [
        "![](https://miro.medium.com/max/922/1*P7xpVKDJfO9k4of2n5tfoQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aE69DizqpoLb"
      },
      "source": [
        "No dataset do viroma, essas técnicas de redução de dimensionalidade não dão resultados muito melhores do que o PCA. Isso depende muito do dataset e da presença de não linearidades nas relações entre as features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1BC5xK4poLb"
      },
      "source": [
        "from sklearn.manifold import Isomap, TSNE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t6csLyWpoLb"
      },
      "source": [
        "proj_isomap = Isomap().fit_transform(np.log10(df.iloc[:,2:]))\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.scatterplot(x=proj_isomap[:,0], y=proj_isomap[:,1], hue=df[\"Material\"])\n",
        "plt.title(\"Redução de dimensionalidade com Isomap\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOSwhD9cpoLb"
      },
      "source": [
        "proj_tsne = TSNE().fit_transform(np.log10(df.iloc[:,2:]))\n",
        "plt.figure(figsize=(10,7))\n",
        "sns.scatterplot(x=proj_tsne[:,0], y=proj_tsne[:,1], hue=df[\"Material\"])\n",
        "plt.title(\"Redução de dimensionalidade com T-SNE\");"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MklGuTEtpoLb"
      },
      "source": [
        "Vamos dar um exemplo mais clássico (porém menos biológico) de aplicação desses métodos. O próprio Scikit-learn tem um dataset de imagens escaneadas de dígitos escritos à mão, em resolução superbaixa (8x8). Cada variável corresponde a um pixel (0=preto, 16=branco). Esse dataset é muito usado para testar algoritmos de classificação (que não são nosso foco agora)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TbzAXTtpoLd"
      },
      "source": [
        "from sklearn.datasets import load_digits\n",
        "pixels, numbers = load_digits(return_X_y=True, as_frame=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrxUSPACpoLd"
      },
      "source": [
        "plt.imshow(pixels.values[5].reshape((8,8)), cmap=plt.cm.gray_r)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbCRAzvwpoLe"
      },
      "source": [
        "pca = PCA(0.80)\n",
        "pca.fit(pixels)\n",
        "transformed = pca.transform(pixels)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.title(\"Representação 2D com PCA\")\n",
        "sns.scatterplot(x=transformed[:,0], y=transformed[:,1],hue=numbers.astype(\"category\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlbPFjYzpoLe"
      },
      "source": [
        "isomap = Isomap()\n",
        "transformed = isomap.fit_transform(pixels)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.title(\"Representação 2D com Isomap\")\n",
        "sns.scatterplot(x=transformed[:,0], y=transformed[:,1],hue=numbers.astype(\"category\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NId690ppoLe"
      },
      "source": [
        "tsne = TSNE()\n",
        "transformed_digits = tsne.fit_transform(pixels)\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.title(\"Representação 2D com T-SNE\")\n",
        "sns.scatterplot(transformed_digits[:,0], transformed_digits[:,1], hue=numbers.astype(\"category\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RHZ_jRGpoLe"
      },
      "source": [
        "## Clustering\n",
        "Uma parte muito importante da aprendizagem não supervisionada são os chamados algoritmos de clustering, que nos permitem agrupar as instâncias com base em similaridade. Como visto na aula teórica, o algoritmo mais simples para essa tarefa é o **K-Means**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLdcEqGZpoLe"
      },
      "source": [
        "from sklearn.cluster import KMeans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkRB8tUapoLe"
      },
      "source": [
        "kmeans = KMeans(n_clusters=10)\n",
        "cluster = kmeans.fit_predict(transformed_digits)\n",
        "cluster = pd.Series(cluster, dtype=\"category\", name=\"cluster\")\n",
        "sns.scatterplot(x=transformed_digits[:,0], y=transformed_digits[:,1], hue=cluster)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlB5tdSSpoLe"
      },
      "source": [
        "Podemos ver no heatmap a seguir que os clusters encontrados correspondem aos valores dos dígitos escaneados. Isso é uma demonstração da capacidade desses algoritmos de captar lógica subjacente aos dados: mesmo em um contexto não supervisionado, foi possível encontrar os grupos relevantes nesse dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM7dZSWxpoLf"
      },
      "source": [
        "sns.clustermap(pd.crosstab(cluster, numbers), cmap=\"mako\", annot=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjPL34_EpoLg"
      },
      "source": [
        "Outro algoritmo interessante é o GMM. Esse algoritmo é um pouco mais flexível que o K-Means (assume clusters elípticos em vez de esféricos) e tem resultados probabilísticos. Isso é relevante pois dá uma ideia da incerteza na associação instância-cluster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-K6A004poLg"
      },
      "source": [
        "from sklearn.mixture import GaussianMixture"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FrZUeUfpoLg"
      },
      "source": [
        "gmm = GaussianMixture(10)\n",
        "cluster = gmm.fit_predict(transformed_digits)\n",
        "cluster = pd.Series(cluster, dtype=\"category\", name=\"cluster\")\n",
        "plt.figure(figsize=(15,7))\n",
        "sns.scatterplot(x=transformed_digits[:,0], y=transformed_digits[:,1], hue=cluster)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5lHxK8WepoLh"
      },
      "source": [
        "gmm.predict_proba(transformed_digits)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLluBwVPpoLh"
      },
      "source": [
        "gmm.predict_proba(transformed_digits).max(axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lKsUXAVpoLh"
      },
      "source": [
        "plt.figure(figsize=(15,7))\n",
        "sns.scatterplot(x=transformed_digits[:,0], y=transformed_digits[:,1], hue=gmm.predict_proba(transformed_digits).max(axis=1),\n",
        "               palette=\"RdBu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crW0Ru_ZpoLh"
      },
      "source": [
        "Note que essa discussão não esgota as possibilidades em análise de clusters: há um grande número de algoritmos disponíveis, cada um com seus prós e contras. A imagem a seguir, extraída da documentação do Scikit-learn, traz os resultados de diversos algoritmos em diversos datasets, mostrando na prática as vantagens e desvantagens de cada um deles:\n",
        "\n",
        "![](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_0011.png)"
      ]
    }
  ]
}